{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Prepare the dataset:\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the path to the dataset folder\n",
    "data_path = \"C:/Users/Administrator/Desktop/640eb91e5f67b_Problem_Statement__Invoice/inv_train\"\n",
    "\n",
    "# Get the list of image filenames\n",
    "image_files = [os.path.join(data_path, file) for file in os.listdir(data_path) if file.endswith('.jpeg')]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_files, test_files = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the list of filenames to a csv file\n",
    "pd.DataFrame({'filename': train_files}).to_csv('train.csv', index=False)\n",
    "pd.DataFrame({'filename': test_files}).to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Annoting the data\n",
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Set the path to the dataset folder\n",
    "data_path = \"C:/Users/Administrator/Desktop/640eb91e5f67b_Problem_Statement__Invoice/inv_train\"\n",
    "\n",
    "# Parse the xml files and extract the image filenames, bounding box coordinates, and labels\n",
    "rows = []\n",
    "for file in os.listdir(data_path):\n",
    "    if file.endswith('.xml'):\n",
    "        tree = ET.parse(os.path.join(data_path, file))\n",
    "        root = tree.getroot()\n",
    "        filename = root.find('filename').text\n",
    "        for obj in root.iter('object'):\n",
    "            label = obj.find('name').text\n",
    "            xmin = int(obj.find('bndbox').find('xmin').text)\n",
    "            ymin = int(obj.find('bndbox').find('ymin').text)\n",
    "            xmax = int(obj.find('bndbox').find('xmax').text)\n",
    "            ymax = int(obj.find('bndbox').find('ymax').text)\n",
    "            rows.append([os.path.join(data_path, filename), xmin, ymin, xmax, ymax, label])\n",
    "\n",
    "# Save the annotations to a csv file\n",
    "df = pd.DataFrame(rows, columns=['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'label'])\n",
    "df.to_csv('train_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python C:/Users/Administrator/Downloads/yolov5-master/yolov5-master/train.py --img 640 --batch 16 --epochs 100 --data data.yaml --cfg models/yolov5s.yaml --weights yolov5s.pt --name invoice_extraction --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\Administrator/.cache\\torch\\hub\\master.zip\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "custom() got an unexpected keyword argument 'path_or_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-3c27795db73f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;31m# Define YOLOv5 model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ultralytics/yolov5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"custom\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_or_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"/yolov5s.pt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\torch\\hub.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    556\u001b[0m                                            verbose=verbose, skip_validation=skip_validation)\n\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\torch\\hub.py\u001b[0m in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mentry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: custom() got an unexpected keyword argument 'path_or_model'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Define dataset class\n",
    "class InvoiceDataset(Dataset):\n",
    "    def __init__(self, data_path, label_path, transforms=None):\n",
    "        self.data_path = data_path\n",
    "        self.transforms = transforms\n",
    "        self.labels = pd.read_csv(label_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load image and label\n",
    "        image_path = os.path.join(self.data_path, self.labels.iloc[index, 0])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = self.labels.iloc[index, 1:]\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "data_path = \"C:/Users/Administrator/Desktop/640eb91e5f67b_Problem_Statement__Invoice/inv_train\"\n",
    "label_path = \"invoice_labels.csv\"\n",
    "df_labels = pd.read_csv(label_path)\n",
    "train_labels, test_labels = train_test_split(df_labels, test_size=0.2, random_state=42)\n",
    "train_labels.to_csv(\"train_labels.csv\", index=False)\n",
    "test_labels.to_csv(\"test_labels.csv\", index=False)\n",
    "\n",
    "# Define transforms for image augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=(-10, 10)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = InvoiceDataset(data_path, \"train_labels.csv\", transform)\n",
    "test_dataset = InvoiceDataset(data_path, \"test_labels.csv\", transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Define YOLOv5 model\n",
    "model = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path_or_model=\"/yolov5s.pt\", force_reload=True)\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        # Send images and targets to device\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = outputs.loss\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        model.optimizer.zero_grad()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"invoice_extraction_model.pt\")\n",
    "\n",
    "# Inference on the test dataset\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(\"invoice_extraction_model.pt\"))\n",
    "results = []\n",
    "for batch_idx, (images, targets) in enumerate(test_loader):\n",
    "    # Send images to device\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "\n",
    "    # Post-process predictions\n",
    "    boxes = outputs.xy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
